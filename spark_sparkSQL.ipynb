{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark and SparkSQL in Python\n",
    "In this note, I demonstrate Apache Spark and SparkSQL in Python, using PySpark library.\n",
    "\n",
    "Apache Spark is an open-source library for processing large-scale data. It provides an interfrace for handling data over a cluster of processing machines so that the data can be processed in parallel and fault-tolerant manner. As in many other parallel data processing scheme, Spark also uses an algorithm called MapReduce. Yet, Spark has an advantage over schemes using the naive MapReduce algorithm. This adavantage comes from that Spark uses a data structure called Resilient Distributed Datasets (RDD). When the task requires multiple iterations of MapReduce procedures, the data should be stored to and read from stable storage system such as HDFS (Hadoop Distributed File System) for each iteration, and it takes the most of computing time. RDD is the data structure to store these intermediate results over the distributed memory (RAM) intead of the disk. Since the data processing is much faster on RAM than the disk drive, this RDD file system have speeded up the parallel data processing significantly.\n",
    "\n",
    "As Spark continuosly improves, Spark dataframe is introduced to deal with more structured data. While Spark dataframe is also a dataset distributed over memory as RDD, it is organized along with columns as in tables of relational database. For further convenient handing of data in Spark dataframe, Spark provides a wrapper called SparkSQL. SparkSQL allows the data in Spark dataframe can be handled with SQL-like commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing with PySpark\n",
    "I load PySpark here. I also Pandas for comparision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration, I use [credit card fraud dataset](https://github.com/GuansongPang/anomaly-detection-datasets) which I used in my [anomaly detection note](anomaly_detection_PCA.ipynb). This is the dataset of 284807 data with 29 features (\"V1\"~\"V28\",\"amount\") and a label (\"class\") presenting if the data is fraudulent (1 being fraud, 0 being not). I take a sample of the dataset with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 284807\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.935192</td>\n",
       "      <td>0.766490</td>\n",
       "      <td>0.881365</td>\n",
       "      <td>0.313023</td>\n",
       "      <td>0.763439</td>\n",
       "      <td>0.267669</td>\n",
       "      <td>0.266815</td>\n",
       "      <td>0.786444</td>\n",
       "      <td>0.475312</td>\n",
       "      <td>0.510600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561184</td>\n",
       "      <td>0.522992</td>\n",
       "      <td>0.663793</td>\n",
       "      <td>0.391253</td>\n",
       "      <td>0.585122</td>\n",
       "      <td>0.394557</td>\n",
       "      <td>0.418976</td>\n",
       "      <td>0.312697</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.978542</td>\n",
       "      <td>0.770067</td>\n",
       "      <td>0.840298</td>\n",
       "      <td>0.271796</td>\n",
       "      <td>0.766120</td>\n",
       "      <td>0.262192</td>\n",
       "      <td>0.264875</td>\n",
       "      <td>0.786298</td>\n",
       "      <td>0.453981</td>\n",
       "      <td>0.505267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557840</td>\n",
       "      <td>0.480237</td>\n",
       "      <td>0.666938</td>\n",
       "      <td>0.336440</td>\n",
       "      <td>0.587290</td>\n",
       "      <td>0.446013</td>\n",
       "      <td>0.416345</td>\n",
       "      <td>0.313423</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.935217</td>\n",
       "      <td>0.753118</td>\n",
       "      <td>0.868141</td>\n",
       "      <td>0.268766</td>\n",
       "      <td>0.762329</td>\n",
       "      <td>0.281122</td>\n",
       "      <td>0.270177</td>\n",
       "      <td>0.788042</td>\n",
       "      <td>0.410603</td>\n",
       "      <td>0.513018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565477</td>\n",
       "      <td>0.546030</td>\n",
       "      <td>0.678939</td>\n",
       "      <td>0.289354</td>\n",
       "      <td>0.559515</td>\n",
       "      <td>0.402727</td>\n",
       "      <td>0.415489</td>\n",
       "      <td>0.311911</td>\n",
       "      <td>0.014739</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.941878</td>\n",
       "      <td>0.765304</td>\n",
       "      <td>0.868484</td>\n",
       "      <td>0.213661</td>\n",
       "      <td>0.765647</td>\n",
       "      <td>0.275559</td>\n",
       "      <td>0.266803</td>\n",
       "      <td>0.789434</td>\n",
       "      <td>0.414999</td>\n",
       "      <td>0.507585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559734</td>\n",
       "      <td>0.510277</td>\n",
       "      <td>0.662607</td>\n",
       "      <td>0.223826</td>\n",
       "      <td>0.614245</td>\n",
       "      <td>0.389197</td>\n",
       "      <td>0.417669</td>\n",
       "      <td>0.314371</td>\n",
       "      <td>0.004807</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.938617</td>\n",
       "      <td>0.776520</td>\n",
       "      <td>0.864251</td>\n",
       "      <td>0.269796</td>\n",
       "      <td>0.762975</td>\n",
       "      <td>0.263984</td>\n",
       "      <td>0.268968</td>\n",
       "      <td>0.782484</td>\n",
       "      <td>0.490950</td>\n",
       "      <td>0.524303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561327</td>\n",
       "      <td>0.547271</td>\n",
       "      <td>0.663392</td>\n",
       "      <td>0.401270</td>\n",
       "      <td>0.566343</td>\n",
       "      <td>0.507497</td>\n",
       "      <td>0.420561</td>\n",
       "      <td>0.317490</td>\n",
       "      <td>0.002724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0  0.935192  0.766490  0.881365  0.313023  0.763439  0.267669  0.266815   \n",
       "1  0.978542  0.770067  0.840298  0.271796  0.766120  0.262192  0.264875   \n",
       "2  0.935217  0.753118  0.868141  0.268766  0.762329  0.281122  0.270177   \n",
       "3  0.941878  0.765304  0.868484  0.213661  0.765647  0.275559  0.266803   \n",
       "4  0.938617  0.776520  0.864251  0.269796  0.762975  0.263984  0.268968   \n",
       "\n",
       "         V8        V9       V10  ...       V21       V22       V23       V24  \\\n",
       "0  0.786444  0.475312  0.510600  ...  0.561184  0.522992  0.663793  0.391253   \n",
       "1  0.786298  0.453981  0.505267  ...  0.557840  0.480237  0.666938  0.336440   \n",
       "2  0.788042  0.410603  0.513018  ...  0.565477  0.546030  0.678939  0.289354   \n",
       "3  0.789434  0.414999  0.507585  ...  0.559734  0.510277  0.662607  0.223826   \n",
       "4  0.782484  0.490950  0.524303  ...  0.561327  0.547271  0.663392  0.401270   \n",
       "\n",
       "        V25       V26       V27       V28    Amount  class  \n",
       "0  0.585122  0.394557  0.418976  0.312697  0.005824      0  \n",
       "1  0.587290  0.446013  0.416345  0.313423  0.000105      0  \n",
       "2  0.559515  0.402727  0.415489  0.311911  0.014739      0  \n",
       "3  0.614245  0.389197  0.417669  0.314371  0.004807      0  \n",
       "4  0.566343  0.507497  0.420561  0.317490  0.002724      0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd=pd.read_csv(\"fraud_normalized.csv\")\n",
    "print(\"Dataset size:\",len(df_pd))\n",
    "df_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read this dataset and convert it into a RDD file, I first construct the Spark context. While Spark needs multiple machines to demonstrate its power, here I use only my local machine.\n",
    "\n",
    "Now suppose I am interested in summing up the \"V4\" feature for fradulent data (i.e., \"class\"=1). However the data are stored in the format of string by reading the file directly using 'textFile'. To deal with this file, I have to parse the necessary data. To do this, I proceed:\n",
    "* map: split the string into different fields using that the file is CSV.\n",
    "* filter: remove header from the data since it is not numeric.\n",
    "* filter: collect fradulent data only.\n",
    "* map: Take V4 feature and converting themm to numeric data.\n",
    "\n",
    "Up to this part, I obtain \"V4\" features of 492 fradulent data in a RDD file. To sum the data up, I use reduce function with summation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492\n",
      "223.01106799999997\n"
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext(\"local[*]\")\n",
    "\n",
    "rdd_txt=sc.textFile(\"fraud_normalized.csv\")\n",
    "anomaly=rdd_txt.map(lambda r: r.split(\",\")) \\\n",
    "            .filter(lambda r: len(r[29])<4) \\\n",
    "            .filter(lambda r: int(r[29])>0) \\\n",
    "            .map(lambda r: float(r[3]))\n",
    "print(anomaly.count())\n",
    "anomaly_sum = anomaly.reduce(lambda v1,v2:v1+v2)\n",
    "print(anomaly_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSQL with PySpark\n",
    "While one can read the file in string format with PySparc context, it is inconvenient to manually parse the dataset from string data. Instead, one can use Spark dataframe.\n",
    "\n",
    "To create Spark dataframe with PySpark, one need to use the module for SparkSQL. To acess this module, I construct SQL context from PySpark context contructed previously. With this SQL context, I read the CSV file directly to the Spark dataframe. Note that 'inferSchema' option enables to load data in numeric form instead of string and 'header' option allows to interpret the first row of the dataset as the header, rather than a line of data. I present first 5 rows of the dataset, which is identical with the sample shown with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-----+\n",
      "|      V1|      V2|      V3|      V4|      V5|      V6|      V7|      V8|      V9|     V10|     V11|     V12|     V13|     V14|     V15|     V16|     V17|     V18|     V19|     V20|     V21|     V22|     V23|     V24|     V25|     V26|     V27|     V28|  Amount|class|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-----+\n",
      "|0.935192| 0.76649|0.881365|0.313023|0.763439|0.267669|0.266815|0.786444|0.475312|  0.5106|0.252484|0.680908|0.371591|0.635591|0.446084|0.434392|0.737173|0.655066|0.594863|0.582942|0.561184|0.522992|0.663793|0.391253|0.585122|0.394557|0.418976|0.312697|0.005824|    0|\n",
      "|0.978542|0.770067|0.840298|0.271796| 0.76612|0.262192|0.264875|0.786298|0.453981|0.505267|0.381188|0.744342| 0.48619|0.641219| 0.38384|0.464105|0.727794|0.640681| 0.55193| 0.57953| 0.55784|0.480237|0.666938| 0.33644| 0.58729|0.446013|0.416345|0.313423| 1.05E-4|    0|\n",
      "|0.935217|0.753118|0.868141|0.268766|0.762329|0.281122|0.270177|0.788042|0.410603|0.513018|0.322422|0.706683|0.503854|0.640473|0.511697|0.357443|0.763381|0.644945|0.386683|0.585855|0.565477| 0.54603|0.678939|0.289354|0.559515|0.402727|0.415489|0.311911|0.014739|    0|\n",
      "|0.941878|0.765304|0.868484|0.213661|0.765647|0.275559|0.266803|0.789434|0.414999|0.507585|0.271817| 0.71091|0.487635|0.636372|0.289124|0.415653|0.711253|0.788492|0.467058| 0.57805|0.559734|0.510277|0.662607|0.223826|0.614245|0.389197|0.417669|0.314371|0.004807|    0|\n",
      "|0.938617| 0.77652|0.864251|0.269796|0.762975|0.263984|0.268968|0.782484| 0.49095|0.524303|0.236355|0.724477|0.552509|0.608406|0.349419|0.434995|0.724243|0.650665| 0.62606|0.584615|0.561327|0.547271|0.663392| 0.40127|0.566343|0.507497|0.420561| 0.31749|0.002724|    0|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlc=pyspark.sql.SQLContext(sc)\n",
    "df=sqlc.read.csv(\"fraud_normalized.csv\",inferSchema=True,header=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL allows the MapReduce procedures on Spark dataframes to be performed with SQL-like interface. It even allows to make a SQL query on the dataframes in string format. Here I repeat the previous task of summing \"V4\" up for fradulent data. I use 'GROUP BY' to sum the data along each class, and it gives the same result as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------------------+\n",
      "|class| count|          total_V4|\n",
      "+-----+------+------------------+\n",
      "|    1|   492|223.01106799999997|\n",
      "|    0|284315| 71528.47105600013|\n",
      "+-----+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable(\"table\")\n",
    "\n",
    "V4=sqlc.sql(\"\"\"\n",
    "    SELECT class, COUNT(V4) AS count, SUM(V4) AS total_V4\n",
    "    FROM table\n",
    "    GROUP BY class\n",
    "\"\"\")\n",
    "V4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "The explanation for concepts of Apache Spark, RDD, Spark dataframe is based on [this](https://www.tutorialspoint.com/apache_spark/apache_spark_rdd.htm) and [this](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html) references. Documentation of Spary Python API can be found in [here](https://spark.apache.org/docs/latest/api/python/index.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
